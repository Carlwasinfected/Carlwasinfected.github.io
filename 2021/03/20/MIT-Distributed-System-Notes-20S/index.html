<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Carl">





<title>MIT-Distributed-System-Notes-20S | Hexo</title>



    <link rel="icon" href="/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
    


<meta name="generator" content="Hexo 5.4.0"></head>
<body>
    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">Carl&#39;s Blog</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">Carl&#39;s Blog</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
        <div class="main">
            <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    document.ready(
        function () {
            tocbot.init({
                tocSelector: '.tocbot-list',
                contentSelector: '.post-content',
                headingSelector: 'h1, h2, h3, h4, h5',
                collapseDepth: 1,
                orderedList: false,
                scrollSmooth: true,
            })
        }
    )

    function expand_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 6,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "collapse_toc()");
        b.innerHTML = "Collapse all"
    }

    function collapse_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 1,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "expand_toc()");
        b.innerHTML = "Expand all"
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">MIT-Distributed-System-Notes-20S</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">Carl</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">March 20, 2021&nbsp;&nbsp;20:39:42</a>
                        </span>
                    
                    
                </div>
            
        </header>

        <div class="post-content">
            <p><a target="_blank" rel="noopener" href="http://nil.csail.mit.edu/6.824/2020/schedule.html">Course Schedule - 2020 Spring</a></p>
<blockquote>
<p>** Performance</p>
<p>** Fault Tolerance  – 高容错 &lt;=&gt; clusters中总会随时有instnace发生错误 e.g.,断电 无响应…</p>
<ul>
<li><p>Availability </p>
<blockquote>
<p>Goal：在有一定量错误的情况下，整个system不会crash，还能正常运行</p>
</blockquote>
</li>
<li><p>Recoverability</p>
<blockquote>
<p>Goal：监听，如果错误数量大于一个阈值就停止工作，但在一定数量错误被解决后可恢复工作</p>
<p>Howto：</p>
<pre><code>     + NV Storage 比如flash、硬盘    
     +  Replication机制 比如Raft，维护超过一个随时都**identical** 的副本
</code></pre>
</blockquote>
</li>
</ul>
<p>** Consistency</p>
<ul>
<li>Strong Cons – too expensive</li>
<li>Weak Cons</li>
</ul>
</blockquote>
<h1 id="LEC-1-Intros"><a href="#LEC-1-Intros" class="headerlink" title="LEC 1 Intros"></a>LEC 1 Intros</h1><ul>
<li><p>课前Paper：<a href="/Users/carl/Desktop/proj/MIT-Distributed-System/Papers/MapReduce2004.pdf">MapReduce from Google in 2004</a></p>
</li>
<li><p>讲义：<a target="_blank" rel="noopener" href="https://pdos.csail.mit.edu/6.824/notes/l01.txt">https://pdos.csail.mit.edu/6.824/notes/l01.txt</a></p>
  <figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br></pre></td><td class="code"><pre><span class="line">6.824 2021 Lecture 1: Introduction</span><br><span class="line"></span><br><span class="line">6.824: Distributed Systems Engineering</span><br><span class="line"></span><br><span class="line">What is a distributed system?</span><br><span class="line">  multiple cooperating computers</span><br><span class="line">  storage for big web sites, MapReduce, peer-to-peer sharing, &amp;c</span><br><span class="line">  lots of critical infrastructure is distributed</span><br><span class="line"></span><br><span class="line">Why do people build distributed systems?</span><br><span class="line">  to increase capacity via parallelism</span><br><span class="line">  to tolerate faults via replication</span><br><span class="line">  to place computing physically close to external entities</span><br><span class="line">  to achieve security via isolation</span><br><span class="line"></span><br><span class="line">But:</span><br><span class="line">  many concurrent parts, complex interactions</span><br><span class="line">  must cope with partial failure</span><br><span class="line">  tricky to realize performance potential</span><br><span class="line"></span><br><span class="line">Why take this course?</span><br><span class="line">  interesting -- hard problems, powerful solutions</span><br><span class="line">  used by real systems -- driven by the rise of big Web sites</span><br><span class="line">  active research area -- important unsolved problems</span><br><span class="line">  hands-on -- you&#x27;ll build real systems in the labs</span><br><span class="line"></span><br><span class="line">COURSE STRUCTURE</span><br><span class="line"></span><br><span class="line">http://pdos.csail.mit.edu/6.824</span><br><span class="line"></span><br><span class="line">Course staff:</span><br><span class="line">  Frans Kaashoek, lecturer</span><br><span class="line">  Lily Tsai, TA</span><br><span class="line">  Cel Skeggs, TA</span><br><span class="line">  David Morejon, TA</span><br><span class="line">  Jose Javier Gonzalez, TA  </span><br><span class="line"></span><br><span class="line">Course components:</span><br><span class="line">  lectures</span><br><span class="line">  papers</span><br><span class="line">  two exams</span><br><span class="line">  labs</span><br><span class="line">  final project (optional)</span><br><span class="line"></span><br><span class="line">Lectures:</span><br><span class="line">  big ideas, paper discussion, and labs</span><br><span class="line">  will be video-taped, available online</span><br><span class="line"></span><br><span class="line">Papers:</span><br><span class="line">  research papers, some classic, some new</span><br><span class="line">  problems, ideas, implementation details, evaluation</span><br><span class="line">  many lectures focus on papers</span><br><span class="line">  please read papers before class!</span><br><span class="line">  each paper has a short question for you to answer</span><br><span class="line">  and we ask you to send us a question you have about the paper</span><br><span class="line">  submit question&amp;answer before start of lecture</span><br><span class="line"></span><br><span class="line">Exams:</span><br><span class="line">  Mid-term exam in class</span><br><span class="line">  Final exam during finals week</span><br><span class="line">  Mostly about papers and labs</span><br><span class="line"></span><br><span class="line">Labs:</span><br><span class="line">  goal: deeper understanding of some important techniques</span><br><span class="line">  goal: experience with distributed programming</span><br><span class="line">  first lab is due a week from Friday</span><br><span class="line">  one per week after that for a while</span><br><span class="line"></span><br><span class="line">Lab 1: MapReduce</span><br><span class="line">Lab 2: replication for fault-tolerance using Raft</span><br><span class="line">Lab 3: fault-tolerant key/value store</span><br><span class="line">Lab 4: sharded key/value store</span><br><span class="line"></span><br><span class="line">Optional final project at the end, in groups of 2 or 3.</span><br><span class="line">  The final project substitutes for Lab 4.</span><br><span class="line">  You think of a project and clear it with us.</span><br><span class="line">  Code, short write-up, short demo on last day.</span><br><span class="line"></span><br><span class="line">Lab grades depend on how many test cases you pass</span><br><span class="line">  we give you the tests, so you know whether you&#x27;ll do well</span><br><span class="line"></span><br><span class="line">Debugging the labs can be time-consuming</span><br><span class="line">  start early</span><br><span class="line">  come to TA office hours</span><br><span class="line">  ask questions on Piazza</span><br><span class="line"></span><br><span class="line">MAIN TOPICS</span><br><span class="line"></span><br><span class="line">This is a course about infrastructure for applications.</span><br><span class="line"><span class="bullet">  *</span> Storage.</span><br><span class="line"><span class="bullet">  *</span> Communication.</span><br><span class="line"><span class="bullet">  *</span> Computation.</span><br><span class="line"></span><br><span class="line">The big goal: abstractions that hide the complexity of distribution.</span><br><span class="line">  A couple of topics will come up repeatedly in our search.</span><br><span class="line"></span><br><span class="line">Topic: fault tolerance</span><br><span class="line">  1000s of servers, big network -&gt; always something broken</span><br><span class="line"><span class="code">    We&#x27;d like to hide these failures from the application.</span></span><br><span class="line"><span class="code">  We often want:</span></span><br><span class="line"><span class="code">    Availability -- app can make progress despite failures</span></span><br><span class="line"><span class="code">    Recoverability -- app will come back to life when failures are repaired</span></span><br><span class="line"><span class="code">  Big idea: replicated servers.</span></span><br><span class="line"><span class="code">    If one server crashes, can proceed using the other(s).</span></span><br><span class="line"><span class="code">    Very hard to get right</span></span><br><span class="line"><span class="code">      server may not have crashed, but just unreachable for some</span></span><br><span class="line"><span class="code">        but still serving requests from clients</span></span><br><span class="line"><span class="code">    Labs 1, 2 and 3</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">Topic: consistency</span></span><br><span class="line"><span class="code">  General-purpose infrastructure needs well-defined behavior.</span></span><br><span class="line"><span class="code">    E.g. &quot;Get(k) yields the value from the most recent Put(k,v).&quot;</span></span><br><span class="line"><span class="code">  Achieving good behavior is hard!</span></span><br><span class="line"><span class="code">    &quot;Replica&quot; servers are hard to keep identical.</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">Topic: performance</span></span><br><span class="line"><span class="code">  The goal: scalable throughput</span></span><br><span class="line"><span class="code">    Nx servers -&gt; Nx total throughput via parallel CPU, disk, net.</span></span><br><span class="line"><span class="code">  Scaling gets harder as N grows:</span></span><br><span class="line"><span class="code">    Load im-balance, stragglers, slowest-of-N latency.</span></span><br><span class="line"><span class="code">    Non-parallelizable code: initialization, interaction.</span></span><br><span class="line"><span class="code">    Bottlenecks from shared resources, e.g. network.</span></span><br><span class="line"><span class="code">  Some performance problems aren&#x27;t easily solved by scaling</span></span><br><span class="line"><span class="code">    e.g. quick response time for a single user request</span></span><br><span class="line"><span class="code">    e.g. all users want to update the same data</span></span><br><span class="line"><span class="code">    often requires better design rather than just more computers</span></span><br><span class="line"><span class="code">  Lab 4</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">Topic: Fault-tolerance, consistency, and performance are enemies.</span></span><br><span class="line"><span class="code">  Strong fault tolerance requires communication</span></span><br><span class="line"><span class="code">    e.g., send data to backup</span></span><br><span class="line"><span class="code">  Strong consistency requires communication,</span></span><br><span class="line"><span class="code">    e.g. Get() must check for a recent Put().</span></span><br><span class="line"><span class="code">  Many designs provide only weak consistency, to gain speed.</span></span><br><span class="line"><span class="code">    e.g. Get() does *not* yield the latest Put()!</span></span><br><span class="line"><span class="code">    Painful for application programmers but may be a good trade-off.</span></span><br><span class="line"><span class="code">  Many design points are possible in the consistency/performance spectrum!</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">Topic: implementation</span></span><br><span class="line"><span class="code">  RPC, threads, concurrency control.</span></span><br><span class="line"><span class="code">  The labs...</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">HISTORICAL CONTEXT</span></span><br><span class="line"><span class="code">  Local-area networks and Internet apps (since 1980s)</span></span><br><span class="line"><span class="code">    10-100s machines: AFS</span></span><br><span class="line"><span class="code">    Internet-scale apps: DNS and Email</span></span><br><span class="line"><span class="code">  Data centers (late 1990s/early 2000s)</span></span><br><span class="line"><span class="code">    Web sites with many users (many millions) and much data</span></span><br><span class="line"><span class="code">      Google, Yahoo, Facebook, Amazon, Microsoft, etc.</span></span><br><span class="line"><span class="code">      Early apps: web search, email, shopping, etc.</span></span><br><span class="line"><span class="code">    Explosion of cool and interesting systems</span></span><br><span class="line"><span class="code">      &gt; 1000s of machines</span></span><br><span class="line"><span class="code">      Systems mostly for internal use, engineers wrote research papers about them</span></span><br><span class="line"><span class="code">  Cloud computing</span></span><br><span class="line"><span class="code">    Users outsourcing computation/storage to cloud providers</span></span><br><span class="line"><span class="code">    Users run their own big web sites on clouds</span></span><br><span class="line"><span class="code">    Users run large computations of lots of data (e.g., machine learning)</span></span><br><span class="line"><span class="code">    =&gt; Much new user-facing distributed systems infrastructure</span></span><br><span class="line"><span class="code">  Current state: very active area of research and development in academia and industry</span></span><br><span class="line"><span class="code">    Hard to keep up with!</span></span><br><span class="line"><span class="code">      Some systems in the 6.824 papers are dated, but concepts are still relevant</span></span><br><span class="line"><span class="code">    6.824: heavy on fault-tolerance/storage</span></span><br><span class="line"><span class="code">      but touches on communication and computation too</span></span><br><span class="line"><span class="code">      </span></span><br><span class="line"><span class="code">CASE STUDY: MapReduce</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">Let&#x27;s talk about MapReduce (MR) as a case study</span></span><br><span class="line"><span class="code">  a good illustration of 6.824&#x27;s main topics</span></span><br><span class="line"><span class="code">  hugely influential</span></span><br><span class="line"><span class="code">  the focus of Lab 1</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">MapReduce overview</span></span><br><span class="line"><span class="code">  context: multi-hour computations on multi-terabyte data-sets</span></span><br><span class="line"><span class="code">    e.g. build search index, or sort, or analyze structure of web</span></span><br><span class="line"><span class="code">    only practical with 1000s of computers</span></span><br><span class="line"><span class="code">    applications not written by distributed systems experts</span></span><br><span class="line"><span class="code">  overall goal: easy for non-specialist programmers</span></span><br><span class="line"><span class="code">  programmer just defines Map and Reduce functions</span></span><br><span class="line"><span class="code">    often fairly simple sequential code</span></span><br><span class="line"><span class="code">  MR takes care of, and hides, all aspects of distribution!</span></span><br><span class="line"><span class="code">  </span></span><br><span class="line"><span class="code">Abstract view of a MapReduce job</span></span><br><span class="line"><span class="code">  input is (already) split into M files</span></span><br><span class="line"><span class="code">  Input1 -&gt; Map -&gt; a,1 b,1</span></span><br><span class="line"><span class="code">  Input2 -&gt; Map -&gt;     b,1</span></span><br><span class="line"><span class="code">  Input3 -&gt; Map -&gt; a,1     c,1</span></span><br><span class="line"><span class="code">                    |   |   |</span></span><br><span class="line"><span class="code">                    |   |   -&gt; Reduce -&gt; c,1</span></span><br><span class="line"><span class="code">                    |   -----&gt; Reduce -&gt; b,2</span></span><br><span class="line"><span class="code">                    ---------&gt; Reduce -&gt; a,2</span></span><br><span class="line"><span class="code">  MR calls Map() for each input file, produces set of k2,v2</span></span><br><span class="line"><span class="code">    &quot;intermediate&quot; data</span></span><br><span class="line"><span class="code">    each Map() call is a &quot;task&quot;</span></span><br><span class="line"><span class="code">  MR gathers all intermediate v2&#x27;s for a given k2,</span></span><br><span class="line"><span class="code">    and passes each key + values to a Reduce call</span></span><br><span class="line"><span class="code">  final output is set of &lt;k2,v3&gt; pairs from Reduce()s</span></span><br><span class="line"><span class="code"># 补充： 1&gt; Input和Output都是Files（GFS中， 不超过64mb的chunks）</span></span><br><span class="line"><span class="code">#				2&gt; Input File &lt;k1,v1&gt; -&gt; Map() -&gt; list(&lt;k2,v2&gt;) ##  intermediate key</span></span><br><span class="line"><span class="code">#				3&gt; The **COLLECTER** of MR 收集所有Map中的所有同K 分发给Reduce()</span></span><br><span class="line"><span class="code">#   		4&gt; list(&lt;k2,v2&gt;) -&gt; Reduce() -&gt; &lt;k2,v3&gt; -&gt; GFS(Output File)</span></span><br><span class="line"><span class="code">				</span></span><br><span class="line"><span class="code">Example: word count</span></span><br><span class="line"><span class="code">  input is thousands of text files</span></span><br><span class="line"><span class="code">  Map(k, v)</span></span><br><span class="line"><span class="code">    split v into words</span></span><br><span class="line"><span class="code">    for each word w</span></span><br><span class="line"><span class="code">      emit(w, &quot;1&quot;)</span></span><br><span class="line"><span class="code">  Reduce(k, v)</span></span><br><span class="line"><span class="code">    emit(len(v))</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">MapReduce scales well:</span></span><br><span class="line"><span class="code">  N &quot;worker&quot; computers get you Nx throughput.</span></span><br><span class="line"><span class="code">    Maps()s can run in parallel, since they don&#x27;t interact.</span></span><br><span class="line"><span class="code">    Same for Reduce()s.</span></span><br><span class="line"><span class="code">  **So you can get more throughput by buying more computers.**</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">MapReduce hides many details:</span></span><br><span class="line"><span class="code">  sending app code to servers</span></span><br><span class="line"><span class="code">  tracking which tasks are done</span></span><br><span class="line"><span class="code">  moving data from Maps to Reduces</span></span><br><span class="line"><span class="code">  balancing load over servers</span></span><br><span class="line"><span class="code">  recovering from failures</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">However, MapReduce limits what apps can do:</span></span><br><span class="line"><span class="code">  No interaction or state (other than via intermediate output).</span></span><br><span class="line"><span class="code">  No iteration, no multi-stage pipelines.</span></span><br><span class="line"><span class="code">  No real-time or streaming processing.</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">Input and output are stored on the GFS cluster file system</span></span><br><span class="line"><span class="code">  MR needs huge parallel input and output throughput.</span></span><br><span class="line"><span class="code">  GFS splits files over many servers, in 64 MB chunks</span></span><br><span class="line"><span class="code">    Maps read in parallel</span></span><br><span class="line"><span class="code">    Reduces write in parallel</span></span><br><span class="line"><span class="code">  GFS also replicates each file on 2 or 3 servers</span></span><br><span class="line"><span class="code">  Having GFS is a big win for MapReduce</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">What will likely limit the performance?</span></span><br><span class="line"><span class="code">  We care since that&#x27;s the thing to optimize.</span></span><br><span class="line"><span class="code">  CPU? memory? disk? network?</span></span><br><span class="line"><span class="code">  In 2004 authors were limited by network capacity.</span></span><br><span class="line"><span class="code">    What does MR send over the network?</span></span><br><span class="line"><span class="code">      Maps read input from GFS.</span></span><br><span class="line"><span class="code">      Reduces read Map output.</span></span><br><span class="line"><span class="code">        Can be as large as input, e.g. for sorting.</span></span><br><span class="line"><span class="code">      Reduces write output files to GFS.</span></span><br><span class="line"><span class="code">    [diagram: servers, tree of network switches]</span></span><br><span class="line"><span class="code">    In MR&#x27;s all-to-all shuffle, half of traffic goes through root switch.</span></span><br><span class="line"><span class="code">    Paper&#x27;s root switch: 100 to 200 gigabits/second, total</span></span><br><span class="line"><span class="code">      1800 machines, so 55 megabits/second/machine.</span></span><br><span class="line"><span class="code">      55 is small, e.g. much less than disk or RAM speed.</span></span><br><span class="line"><span class="code">  Today: networks and root switches are much faster relative to CPU/disk.</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">Some details (paper&#x27;s Figure 1):</span></span><br><span class="line"><span class="code">  one coordinator, that hands out tasks to workers and remembers progress.</span></span><br><span class="line"><span class="code">  1. coordinator gives Map tasks to workers until all Maps complete</span></span><br><span class="line"><span class="code">     *** Maps write output (intermediate data) to local disk ***</span></span><br><span class="line"><span class="code">     *** Maps split output, by hash, into one file per Reduce task ***</span></span><br><span class="line"><span class="code">  2. after all Maps have finished, coordinator hands out Reduce tasks</span></span><br><span class="line"><span class="code">     each Reduce fetches its intermediate output from (all) Map workers</span></span><br><span class="line"><span class="code">     each Reduce task writes a separate output file on GFS</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">How does MR minimize network use?</span></span><br><span class="line"><span class="code">  Coordinator tries to run each Map task on GFS server that stores its input.</span></span><br><span class="line"><span class="code">    All computers run both GFS and MR workers</span></span><br><span class="line"><span class="code">    So input is read from local disk (via GFS), not over network.</span></span><br><span class="line"><span class="code">  Intermediate data goes over network just once.</span></span><br><span class="line"><span class="code">    Map worker writes to local disk.</span></span><br><span class="line"><span class="code">    Reduce workers read directly from Map workers, not via GFS.</span></span><br><span class="line"><span class="code">  Intermediate data partitioned into files holding many keys.</span></span><br><span class="line"><span class="code">    R is much smaller than the number of keys.</span></span><br><span class="line"><span class="code">    Big network transfers are more efficient.</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">How does MR get good load balance?</span></span><br><span class="line"><span class="code">  Wasteful and slow if N-1 servers have to wait for 1 slow server to finish.</span></span><br><span class="line"><span class="code">  But some tasks likely take longer than others.</span></span><br><span class="line"><span class="code">  Solution: many more tasks than workers.</span></span><br><span class="line"><span class="code">    Coordinator hands out new tasks to workers who finish previous tasks.</span></span><br><span class="line"><span class="code">    So no task is so big it dominates completion time (hopefully).</span></span><br><span class="line"><span class="code">    So faster servers do more tasks than slower ones, finish abt the same time.</span></span><br><span class="line"><span class="code">#   能者多劳</span></span><br><span class="line"><span class="code">		</span></span><br><span class="line"><span class="code">What about fault tolerance?</span></span><br><span class="line"><span class="code">  I.e. what if a worker crashes during a MR job?</span></span><br><span class="line"><span class="code">  We want to completely hide failures from the application programmer!</span></span><br><span class="line"><span class="code">  Does MR have to re-run the whole job from the beginning?</span></span><br><span class="line"><span class="code">    Why not?</span></span><br><span class="line"><span class="code">  MR re-runs just the failed Map()s and Reduce()s.</span></span><br><span class="line"><span class="code">    Suppose MR runs a Map twice, one Reduce sees first run&#x27;s output,</span></span><br><span class="line"><span class="code">      another Reduce sees the second run&#x27;s output?</span></span><br><span class="line"><span class="code">    Correctness requires re-execution to yield exactly the same output.</span></span><br><span class="line"><span class="code">    So Map and Reduce must be pure **deterministic functions**:</span></span><br><span class="line"><span class="code">#     deterministic function: 确定性函数--&gt; 参数一旦给定 输出总identical</span></span><br><span class="line"><span class="code">      they are only allowed to look at their arguments.</span></span><br><span class="line"><span class="code">      **no state, no file I/O, no interaction, no external communication.**</span></span><br><span class="line"><span class="code">  What if you wanted to allow non-functional Map or Reduce?</span></span><br><span class="line"><span class="code">    Worker failure would require whole job to be re-executed,</span></span><br><span class="line"><span class="code">      or you&#x27;d need to create synchronized global checkpoints.</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">Details of worker crash recovery:</span></span><br><span class="line"><span class="code">  * Map worker crashes:</span></span><br><span class="line"><span class="code">    coordinator notices worker no longer responds to pings</span></span><br><span class="line"><span class="code">    coordinator knows which Map tasks it ran on that worker</span></span><br><span class="line"><span class="code">      those tasks&#x27; intermediate output is now lost, must be re-created</span></span><br><span class="line"><span class="code">      coordinator tells other workers to run those tasks</span></span><br><span class="line"><span class="code">    can omit re-running if Reduces already fetched the intermediate data</span></span><br><span class="line"><span class="code">  * Reduce worker crashes.</span></span><br><span class="line"><span class="code">    finished tasks are OK -- stored in GFS, with replicas.</span></span><br><span class="line"><span class="code">    coordinator re-starts worker&#x27;s unfinished tasks on other workers.</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">Other failures/problems:</span></span><br><span class="line"><span class="code">  * What if the coordinator gives two workers the same Map() task?</span></span><br><span class="line"><span class="code">    perhaps the coordinator incorrectly thinks one worker died.</span></span><br><span class="line"><span class="code">    it will tell Reduce workers about only one of them.</span></span><br><span class="line"><span class="code">  * What if the coordinator gives two workers the same Reduce() task?</span></span><br><span class="line"><span class="code">    they will both try to write the same output file on GFS!</span></span><br><span class="line"><span class="code">    atomic GFS rename prevents mixing; one complete file will be visible.</span></span><br><span class="line"><span class="code">  * What if a single worker is very slow -- a &quot;straggler&quot;?</span></span><br><span class="line"><span class="code">    perhaps due to flakey hardware.</span></span><br><span class="line"><span class="code">    coordinator starts a second copy of last few tasks.</span></span><br><span class="line"><span class="code">  * What if a worker computes incorrect output, due to broken h/w or s/w?</span></span><br><span class="line"><span class="code">    too bad! MR assumes &quot;fail-stop&quot; CPUs and software.</span></span><br><span class="line"><span class="code">  * What if the coordinator crashes?</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">Current status?</span></span><br><span class="line"><span class="code">  Hugely influential (Hadoop, Spark, &amp;c).</span></span><br><span class="line"><span class="code">  Probably no longer in use at Google.</span></span><br><span class="line"><span class="code">    Replaced by Flume / FlumeJava (see paper by Chambers et al).</span></span><br><span class="line"><span class="code">    GFS replaced by Colossus (no good description), and BigTable.</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">Conclusion</span></span><br><span class="line"><span class="code">  MapReduce single-handedly made big cluster computation popular.</span></span><br><span class="line"><span class="code">  - Not the most efficient or flexible.</span></span><br><span class="line"><span class="code">  + Scales well.  **扩展性很强**</span></span><br><span class="line"><span class="code">  + Easy to program -- failures and data movement are hidden.**编码层面屏蔽了很多细节**</span></span><br><span class="line"><span class="code">  </span></span><br><span class="line"><span class="code">  These were good trade-offs in practice.</span></span><br><span class="line"><span class="code">  We&#x27;ll see some more advanced successors later in the course.</span></span><br><span class="line"><span class="code">  Have fun with the lab!</span></span><br></pre></td></tr></table></figure></li>
<li><p>Why Distributed System(Huge Computer Clusters)?</p>
<ul>
<li><strong>Parallelism</strong></li>
<li><strong>Less Tolerate Faults</strong></li>
<li>Existing Physical Reasons</li>
<li>Security Issues –&gt; Isolated</li>
</ul>
</li>
<li><p>Why Distributed System is <strong>Challenging</strong></p>
<ul>
<li><p>Concurrency - large number</p>
</li>
<li><p>Partial Failure</p>
</li>
<li><p>Performance</p>
<ul>
<li><p>Scalability</p>
<blockquote>
<p>——”2倍数量的机器就能直接达到2倍的吞吐量吗？“</p>
<p>——“<strong>NO</strong> 很多时候Bottleneck仍然不会解除” </p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Labs</p>
<ul>
<li><p>Lab1：MapReduce</p>
</li>
<li><p>Lab2：Raft算法</p>
<blockquote>
<p>一种实现tolerate faults的算法</p>
</blockquote>
</li>
<li><p>Lab3：利用lab2 build a key-value server with  fault tolerant</p>
</li>
<li><p>Lab4: Sharding KV server </p>
</li>
</ul>
</li>
<li><p>Infra中的应用涉及</p>
<ul>
<li><strong>Storage</strong> – 广泛 直白</li>
<li>Communication –通信</li>
<li>Computation  –i.e., MapReduce</li>
</ul>
<blockquote>
<p>**Some Implementions  of  Distributed System below：</p>
<p>RPC  –&gt; 忽略网络链路不稳定的事实 </p>
<p>threads</p>
<p>concurrency - lock</p>
</blockquote>
</li>
</ul>
<h1 id="LEC2-GO-amp-RPC"><a href="#LEC2-GO-amp-RPC" class="headerlink" title="LEC2 GO &amp; RPC"></a>LEC2 GO &amp; RPC</h1><ul>
<li><p>Go是<em>类型安全</em>与<em>内存安全</em>的， Threads 在Go中叫做<code>Goroutines</code></p>
</li>
<li><p>Multiple Threads 主要用在：</p>
<ul>
<li>I/O场景  =&gt; 高并发 <em>High Concurrency</em></li>
<li>并行场景 =&gt; 多核CPU</li>
<li>易用性 =&gt; MP框架中Master发送<code>ping</code>确认Worker Server是否存活</li>
</ul>
<blockquote>
<p>an alternative way:  <em>event-driven programming</em></p>
<blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1. write code that explicitly interleaves activities, in a single thread. Usually called &quot;event-driven.&quot;</span><br><span class="line">2. Keep a table of state about each activity, e.g. each client request.</span><br><span class="line"></span><br><span class="line">** Eliminates Thread Costs. But doesnot get multi-core speedup **</span><br></pre></td></tr></table></figure></blockquote>
</blockquote>
</li>
<li><p>多线程中每个Thread都有自己独立的<code>程序计数器</code>、<code>寄存器</code>和<code>stack</code>，但实际都还是在同一个Process的地址空间SandBox里（即共享地址空间），只是逻辑上做了隔离。  每个Thread可以通过<em>共享内存</em>的方式读写进行交流，由此引出<em>各种同步问题</em></p>
</li>
<li><p>Threads Challenges:</p>
<ul>
<li><p>共享数据</p>
<blockquote>
<p>高并发下多个线程对于同一内存地址写， 导致<code>Race</code> </p>
<p>common solutions  =&gt; <strong>加锁</strong> 例如Go中的<code>mu.Lock() xxxx//some codes mu.Unlock()  </code></p>
</blockquote>
</li>
<li><p>互相协作 Coordination</p>
<blockquote>
<p>想象一个生产者消费者模型「即 2个线程」 如何通信？</p>
<blockquote>
<ul>
<li>how can the consumer wait (and release the CPU)?</li>
<li>how can the producer wake up the consumer?</li>
</ul>
</blockquote>
<p>==&gt; use <code>Go channels</code> or <code>sync.Cond</code> or <code>WaitGroup</code></p>
</blockquote>
</li>
<li><p>死锁</p>
<blockquote>
<p>Threads之前请求Lock的关系成环</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<ul>
<li><p><a target="_blank" rel="noopener" href="http://nil.csail.mit.edu/6.824/2020/notes/crawler.go"><code>Crawler.go</code></a></p>
<ul>
<li><p>能在<code>Serial(u, fetcher, fetched)</code>前直接加<code>go</code>达到实用协程并行的效果吗？</p>
<blockquote>
<p>不能。 主协程会自动执行，<strong>提早退出</strong> （如下）。</p>
<p>主协程 应用<code>WaitGroup</code>这个<em>数据结构</em>来维护创建的各个<code>goroutine</code> ,API包括<code>Add(n)</code> <code>Done()</code> <code>Wait()</code> . 维护采用<code>引用计数</code>的方式</p>
<p><img src="https://cdn.jsdelivr.net/gh/Carlwasinfected/hximgs@main/data/image-20210411172546155.png" alt="image-20210411172546155"></p>
</blockquote>
</li>
<li><p><code>go run -race xx.go</code> 其中<code>-race</code>是 go 内置的 race 检测器， 可用于检测并行程序是否存在对于<em>共享内存区某些变量</em> 的race </p>
</li>
<li><p>利用<code>channel</code> 这一<code>生产消费者</code>结构可以简化代替通过共享内存 + 加锁防止竞争的方式来读写的方式。 </p>
<ul>
<li><p><code>ch &lt;- []string&#123;&quot;test1&quot;, &quot;test2&quot;&#125;</code>  写入一个元素，该元素为一个 string slice</p>
</li>
<li><p><code>x := &lt;- ch </code> x将会是一个<code>[]string&#123;...&#125;</code></p>
</li>
<li><p><code>for elem := range ch</code> 从管道队列中逐个读出元素（FIFO），并一直轮询接收 goroutine 发送进来的新元素 </p>
<blockquote>
<ul>
<li>如果ch为空 那么程序会一直Block在此处</li>
<li>此句并不是遍历退出，而是一直等待channel中的新元素。 因此需要<code>break</code>退出</li>
<li>读的时候 channel 尾端也可以有新元素不断写入</li>
</ul>
</blockquote>
</li>
<li><p>需不需要手动关闭 channel？</p>
<blockquote>
<p>可以这么做，但很多时候没必要。</p>
<p>在 Go 中 channel 是一个对象。GO GC会一直监视，如果这个channel已经没有了引用，会自动销毁回收</p>
</blockquote>
</li>
</ul>
</li>
<li><p><code>channel</code> 底层实现原理：<a target="_blank" rel="noopener" href="https://blog.csdn.net/QiuHaoqian/article/details/108999754">https://blog.csdn.net/QiuHaoqian/article/details/108999754</a></p>
</li>
<li><p>何时用<code>sharing mem + lock</code> ？ 何时用<code>channel</code>？</p>
<blockquote>
<p>Most problems can be solved in <em>either</em> style</p>
<ul>
<li>What makes the most sense depends on how the programmer thinks<ul>
<li><strong>state</strong> – sharing and locks</li>
<li><strong>communication</strong> – channels</li>
</ul>
</li>
<li>For the 6.824 labs, I recommend sharing+locks for state,<br>and sync.Cond or channels or time.Sleep() for waiting/notification.</li>
</ul>
</blockquote>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>RPC   <a target="_blank" rel="noopener" href="http://nil.csail.mit.edu/6.824/2020/notes/kv.go">demo of Put() &amp; Get()</a></p>
<ul>
<li><p>架构</p>
<blockquote>
<p>client app        handler fns<br>  stub fns          dispatcher<br>   RPC lib            RPC lib<br>   net  &lt;————&gt;  net</p>
</blockquote>
</li>
<li><p>Go RPC lib做了什么？</p>
<blockquote>
<ul>
<li><p>reads each request</p>
</li>
<li><p>creates a new goroutine for this request</p>
</li>
<li><p>unmarshalls request</p>
<blockquote>
<p>解码 反序列化</p>
</blockquote>
</li>
<li><p>looks up the named object (in table create by Register())</p>
</li>
<li><p>calls the object’s named method (dispatch)</p>
</li>
<li><p>marshalls reply</p>
<blockquote>
<p>Go RPC不能传递<code>channels</code>或者<code>function</code></p>
</blockquote>
</li>
<li><p>writes reply on TCP connection</p>
</li>
</ul>
</blockquote>
</li>
<li><p>What About <strong>Failure</strong> when using RPC?</p>
<ul>
<li><p>「best effort」</p>
<blockquote>
<p>client在server response收到前一直阻塞。 如果没收到就产生尝试re-send。 数次不成功后return error</p>
</blockquote>
<p>  这种范式一般只用在幂等的<code>read-only op</code>业务上，会有脏读等问题</p>
</li>
<li><p>「at most once」  – <strong>much better RPC behavior</strong></p>
<blockquote>
<p>对于来自client 的重复请求，server 直接查缓存，返回之前最近的reply，而不是重新run一遍</p>
<p><em>Q: how to detect a duplicate request?</em></p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// client includes **unique ID (XID)** with each request</span></span><br><span class="line"><span class="comment">// client uses **same XID** for re-send</span></span><br><span class="line"><span class="comment">// for server:</span></span><br><span class="line">    <span class="keyword">if</span> seen[xid]:</span><br><span class="line">      r = old[xid]</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">      r = handler()</span><br><span class="line">      old[xid] = r</span><br><span class="line">      seen[xid] = <span class="literal">true</span></span><br></pre></td></tr></table></figure></blockquote>
</li>
</ul>
</li>
<li><p>Errors in RPC </p>
<blockquote>
<p>Go RPC code will return an <strong>error</strong> if it doesn’t get a reply</p>
<p><em>3 Possible reasons below</em></p>
</blockquote>
<ul>
<li>Timeout of execution </li>
<li>Server did not see request</li>
<li>Sever saw &amp; processed the request but server/net carshed before reply came back</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="LEC-3-GFS-–-Google-File-System"><a href="#LEC-3-GFS-–-Google-File-System" class="headerlink" title="LEC 3 GFS – Google File System"></a>LEC 3 GFS – Google File System</h1><blockquote>
<p><em>“Weak Consistency still works fine!”</em></p>
<p>参考详解1: <a target="_blank" rel="noopener" href="https://mr-dai.github.io/gfs/#comments">https://mr-dai.github.io/gfs/#comments</a></p>
<p>参考详解2:<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/354450124">https://zhuanlan.zhihu.com/p/354450124</a></p>
</blockquote>
<ul>
<li><p>为什么构建一个分布式系统如此难？</p>
<blockquote>
<p><strong>high performance</strong> -&gt; shard data over many servers <em>「分片数据」</em><br>  many servers -&gt; constant faults<br>  fault tolerance -&gt; replication<br>  replication -&gt; potential inconsistencies<br>  better consistency -&gt; <strong>low performance</strong> </p>
<p><em>很显然最终结果与我们的初衷截然相反 因此 <code>it is all about trade-offs</code></em></p>
</blockquote>
</li>
<li><p>一个 <code>GFS cluster</code> 的组成『他们都作为一个普通的「用户级进程」运行在<code>Linux</code>机器上』：</p>
<ul>
<li><p><em>一个</em> <code>GFS master</code></p>
<blockquote>
<p>一个集群只有一个 master 使<code>metadata</code>的管理变得简单高效。</p>
<p><strong><code>master</code>的职责？</strong></p>
<blockquote>
<ul>
<li><p>维护<code>元数据</code> <strong>『GFS Cluster的元数据有以下三种』</strong></p>
<blockquote>
<p>元数据都保存在master 的<strong>memory</strong>里,因此操作很快</p>
<p><em>那么memory的大小会成为瓶颈吗？</em></p>
<blockquote>
<p>不太会。因为对于一个 64MB 大小的 Chunk，Master 只需要维持不到 64 字节的元数据。开销很小</p>
</blockquote>
</blockquote>
<ul>
<li>文件与Chunk的Namespace <em>『先写入log 持久化，再操作』</em></li>
<li>文件与Chunk之间的映射关系<em>『先写入log 持久化，再操作』</em></li>
<li>每个Chunk Replica 的位置 <em>『不持久化，仅询问』</em></li>
</ul>
</li>
<li><p>周期性的<code>ping</code>各个 Chunk Server 确认存活 &amp; 更新状态</p>
</li>
<li><p>回收无用的chunk</p>
</li>
</ul>
</blockquote>
</blockquote>
</li>
<li><p>若干甚至上千<code>GFS Chunk Server</code></p>
<blockquote>
<p>Master 通过 HeartBreak 周期性的询问各个 server 所拥有的 Chunk Replica。</p>
<p>注意Master并不把这些信息持久化到 local disk 这样免去了同步数据的开销</p>
</blockquote>
</li>
<li><p>若干<code>GFS Client</code></p>
</li>
</ul>
</li>
<li><p>关于元数据<code>Metadata of Master</code></p>
<ul>
<li><p>在内存<code>RAM</code>中 –&gt; <em>for high speed, it must be <strong>smallish</strong></em></p>
<blockquote>
<ul>
<li><em>filename</em> <em><strong>(NV)</strong></em>–&gt; 维护一个映射表， 包含了每个 file 对应的 `chunk handles 数组</li>
<li><em>chunk handle</em> –&gt;<ul>
<li>version***(NV)***： 当前 chunk 的版本号 – <em>如果不持久化 master 重启后将不知道最新是哪个版本</em></li>
<li>list of chunkservers***(V)***： 持有该 chunk 的所有服务器列表以及数目</li>
<li>primary***(V)***： 哪个 chunk server 是这个 chunk 当前的 primary， 并在此60s周期内持有这个 chunk 的 chunk lease</li>
<li>lease expiration time***(V)***： 租约时间</li>
</ul>
</li>
</ul>
<p><em>NV： 非易失（持久化到硬盘中） V：易失（仅保留在memory中）</em></p>
</blockquote>
</li>
<li><p>在硬盘<code>disk</code>中</p>
<blockquote>
<ul>
<li>log: 执行指令的记录日志</li>
<li>checkpoint：当前 master 完整快照备份（检查点）</li>
</ul>
</blockquote>
</li>
</ul>
</li>
<li><p><code>GFS Cluster</code>如何保证<strong>数据一致性</strong>？</p>
<ul>
<li><p><code>namespace</code>完全由单节点 Master 管理在其<em>内存</em>中，这部分数据的修改可以通过让 Master 为其<strong>添加互斥锁</strong>来解决并发修改的问题，因此命名空间的数据修改是可以确保完全原子的</p>
</li>
<li><p><strong>文件修改后？</strong></p>
<p>  GFS 是<strong>弱一致性</strong>的，并不保证一个 chunk 的所有副本都一致「e.g., 追加失败后的重试」</p>
<p>  首先明确一个<code>file region</code>被修改后可能进入如下<strong>三种状态</strong></p>
<blockquote>
<ul>
<li><code>inconsistent</code> ： 不同的 Client 读不同 Server 中的备份，读结果不同</li>
<li><code>consistent</code>： 不同的 Client 不论读哪个 Replica of Server， 都能得到 identical 的结果</li>
<li><code>defined</code>: 在<code>consistent</code>的基础上，所有的 Client 都可见上一次修改的内容是什么</li>
</ul>
</blockquote>
<p>  状态的变化如下表。</p>
<p>  可见与本次文件修改操作的<strong>执行成功与否</strong> 和<strong>顺序成功还是并发下成功</strong>有关</p>
<blockquote>
<p><code>Write</code>和<code>Record Append</code>是GFS提供的两种<code>文件修改</code>方式API</p>
</blockquote>
<p>  具体来说，如果 修改操作<code>Mutation</code>是<strong>失败</strong>的，那么文件会进入<code>inconsistent</code>状态『比如 primary 和 secondary A 有预期写入的数据，但 secondary B 由于网络丢包没能成功写入』</p>
<p>  反之，如果<code>Mutation</code>是成功的，对于<code>Write</code>操作</p>
<ul>
<li><p>整个过程只有一个 Client 请求写入（即不是并发的）， 那么这部分文件是<code>defined</code>, 同时自然也是<code>consistent</code>。</p>
</li>
<li><p>如果场景是并发的，即该 Primary 同时处理多个 Client发送的写请求，那么</p>
</li>
</ul>
<p>  反之，如果<code>Mutation</code>是成功的，对于<code>Record Append</code>操作『在 chunk 的末尾追加地写入』 tbd</p>
<ul>
<li></li>
</ul>
<p>  <img src="https://cdn.jsdelivr.net/gh/Carlwasinfected/hximgs@main/data/image-20210413014752183.png" alt="image-20210413014752183"></p>
</li>
</ul>
</li>
<li><p>为什么 GFS 设计成只保证弱一致性？ 如何转变为强一致性？</p>
<blockquote>
<p>tbd</p>
</blockquote>
</li>
<li><p><code>GFS Client</code>读取文件的流程</p>
<p>  <img src="https://cdn.jsdelivr.net/gh/Carlwasinfected/hximgs@main/data/image-20210412192814334.png" alt="image-20210412192814334"></p>
<ol>
<li>由上层<code>Application</code>已知指定的文件名和读取位置偏移值，<code>GFS Client</code>可以根据固定的 Chunk 大小『typically 64MB』来计算出目标位置在这个文件的哪一个 Chunk 中『index == 1? index == 2? …』</li>
<li><code>GFS Client</code>向 <code>GFS Master</code> 发出请求，其中包含要读取的文件名<code>filename</code>以及 Chunk 索引值<code>chunk index</code></li>
<li><code>GFS Master</code> 向<code>client</code>响应<code>check handle</code> 以及其所有 Replica 当前所在的<code>location</code>。 同时客户端会以文件名和 Chunk 索引值为键缓存该<code>metadata</code>『注意 client 缓存的是元数据，目的是减少与 master 之前的频繁通信开销  client 不会缓存 chunk 里的文件内容』</li>
<li>之后，客户端便可以选取其中一个 Replica 所在的 Chunk Server 「大多数时是最近的那个」并向其发起请求，请求中会指定需要读取的<code>chunk handle</code> 以及要读取的数据范围<code>byte range</code></li>
<li><code>目标GFS Chunkserver</code>收到请求，返回<code>chunk data</code></li>
</ol>
</li>
<li><p><code>GFS Client</code>写文件的流程</p>
<p>  <img src="https://cdn.jsdelivr.net/gh/Carlwasinfected/hximgs@main/data/image-20210425191442046.png" alt="image-20210425191442046"></p>
<ol>
<li>client 向 master 询问 <code>Primary</code> 和 <code>Secondary</code>。「如果没有对于这个 chunk， 此时没有 chunkserver 持有租约，master 将选择一个授予<code>Lease</code>」</li>
<li>master 返回 <code>Primary</code> 和 <code>Secondary</code> 的信息，client 缓存这些信息，只有当 <code>Primary</code> 不可达或者<strong>租约过期</strong>才再次联系 master；</li>
<li>client 将追加的记录通过<code>DFS</code>发送到**每一个 chunkserver(不仅仅是<code>Primary</code>)**，chunkserver 先将数据写到 LRU 缓存( memory )中(不是local disk)；</li>
<li>一旦 client 确认每个 chunkserver 都收到数据，client 向 <code>Primary</code> 发送写请求，<code>Primary</code> 可能会收到多个连续的写请求，会先将这些操作的顺序写入本地，做好调度排序。</li>
<li><code>Primary</code> 做完写请求后，将写请求和顺序转发给所有的 <code>Secondary</code>，让他们以同样的顺序写数据；</li>
<li><code>Secondary</code> 完成后应答 <code>Primary</code>；</li>
<li><code>Primary</code> 应答 client 成功或失败。如果出现失败，client 会重试，但在重试整个写之前，会先重复步骤 3-7；</li>
</ol>
</li>
<li><p><code>GFS Client</code>追加写的流程</p>
<p>  tbd</p>
</li>
<li><p>GFS 中的快照</p>
</li>
<li><p>GFS 中的<code>fault-torlance</code></p>
</li>
<li><p>FAQ</p>
</li>
</ul>

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>Carl</span>
                    </p>
                
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2021 <a target="_blank" rel="noopener" href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                
                     <p class="copyright-item">
                         <span>Slogan:</span>
                         <span>My Heart is in the <strong>WORK</strong> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i> - from <strong>Andrew Carnegie</strong></i></span>
                     </p>
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/Courses/"># Courses</a>
                    
                        <a href="/tags/System/"># System</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2021/04/02/GolangMeetMe/">GolangMeetMe</a>
            
            
            <a class="next" rel="next" href="/2021/03/18/ImgBlogTesting/">ImgBlogTesting</a>
            
        </section>


    </article>
</div>


<!-- add git talk -->

    <div id="gitalk-container"></div>
    <script src="//cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.js"></script> 
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.5.2/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1.5.2/dist/gitalk.min.js"></script>
<div id="gitalk-container"></div>
<script type="text/javascript">
      var gitalk = new Gitalk({
        clientID: '2cb4dd5456ba994168fe',
        clientSecret: '66d8270f473a683bdc1a1ad00d6d1348b773047b',
        repo: 'Carlwasinfected.github.io',
        owner: 'Carlwasinfected',
        admin: 'Carlwasinfected',
        id: md5(location.pathname),
        labels: 'Gitalk'.split(',').filter(l => l),
        perPage: 15,
        pagerDirection: 'last',
        createIssueManually: true,
        distractionFreeMode: false
      })
      gitalk.render('gitalk-container')
</script>

        </div>
        <footer id="footer" class="footer">
    <div class="copyright">
        <span>© Carl | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a> 
            <br>
        
            <!-- 增加站点统计查询 -->

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<span class="site-uv">
    <i>Vistors:</i>
    <i class="busuanzi-value" id="busuanzi_value_site_uv"></i>
</span>&nbsp;


<span class="site-pv">
    <i>Views:</i>
    <i class="busuanzi-value" id="busuanzi_value_site_pv"></i>
</span>

        </span>
    </div>
</footer>

    </div>
</body>
</html>
